"""
Transcription Page
H·ªó tr·ª£ Whisper v√† PhoWhisper
"""
import streamlit as st
import os
import sys
import tempfile
import soundfile as sf
import torch
import numpy as np
import re

# Add parent directory to path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# Setup FFmpeg tr∆∞·ªõc khi import c√°c module kh√°c
from core.audio.ffmpeg_setup import ensure_ffmpeg
ensure_ffmpeg(silent=True)  # Setup FFmpeg t·ª± ƒë·ªông

from core.asr.model_registry import (
    get_all_models, get_model_info, check_model_dependencies, get_recommended_models
)
from core.asr.transcription_service import (
    load_whisper_model, transcribe_audio, format_transcript
)
from core.asr.phowhisper_service import (
    load_phowhisper_model, transcribe_phowhisper
)
from core.audio.audio_processor import chunk_signal, format_timestamp
from app.components.sidebar import render_sidebar
from app.components.layout import apply_custom_css

# Apply custom CSS
apply_custom_css()

# Ensure session_state keys exist when page is opened directly
for key, default in (
    ("audio_data", None),
    ("audio_sr", None),
    ("audio_info", None),
    ("transcript_result", None),
    ("transcript_text", ""),
):
    if key not in st.session_state:
        st.session_state[key] = default

# Render sidebar with logo
render_sidebar()

st.header("üìù Transcription")

if st.session_state.audio_data is None:
    st.warning("‚ö†Ô∏è Vui l√≤ng upload audio file tr∆∞·ªõc t·∫°i trang 'Upload & Record'")
else:
    # Get all models from registry
    all_models = get_all_models()
    recommended = get_recommended_models()
    
    # Model selection
    st.subheader("üéØ Model Selection")
    
    # Group models by category for better UX
    model_options = []
    model_descriptions = {}
    
    for model_id, info in all_models.items():
        name = info["name"]
        category = info.get("category", "Other")
        is_recommended = info.get("recommended", False)
        vietnamese_support = info.get("vietnamese_support", False)
        
        # Check availability
        is_available, missing_deps = check_model_dependencies(model_id)
        
        # Format display name
        display_name = name
        if is_recommended:
            display_name += " üåü"
        if not vietnamese_support:
            display_name += " (EN)"
        if not is_available:
            display_name += " ‚ö†Ô∏è"
        
        model_options.append((model_id, display_name))
        model_descriptions[model_id] = {
            "description": info.get("description", ""),
            "type": info.get("type", ""),
            "category": category,
            "is_available": is_available,
            "missing_deps": missing_deps,
            "vietnamese_support": vietnamese_support
        }
    
    # Model selection dropdown
    selected_model_id = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh ASR:",
        options=[opt[0] for opt in model_options],
        format_func=lambda x: next(opt[1] for opt in model_options if opt[0] == x),
        help="Ch·ªçn m√¥ h√¨nh ƒë·ªÉ transcribe audio"
    )
    
    # Display model info
    model_info = get_model_info(selected_model_id)
    if model_info:
        col1, col2 = st.columns(2)
        with col1:
            st.markdown(f"**Lo·∫°i:** {model_info['type']}")
            st.markdown(f"**Category:** {model_info['category']}")
        with col2:
            if model_info.get("recommended"):
                st.success("üåü Khuy·∫øn ngh·ªã cho ti·∫øng Vi·ªát")
            if not model_info.get("vietnamese_support"):
                st.warning("‚ö†Ô∏è Ch·ªß y·∫øu cho ti·∫øng Anh")
        
        st.info(f"üí° {model_info.get('description', '')}")
        
        # Check dependencies
        is_available, missing_deps = check_model_dependencies(selected_model_id)
        if not is_available:
            st.error(f"‚ùå Model ch∆∞a s·∫µn s√†ng. Thi·∫øu dependencies: {', '.join(missing_deps)}")
            st.info(f"üí° C√†i ƒë·∫∑t: pip install {' '.join(missing_deps)}")
    
    # Model size selection (if applicable)
    model_size = None
    if model_info and model_info.get("sizes") and len(model_info["sizes"]) > 1:
        default_idx = 0
        if model_info.get("default_size"):
            try:
                default_idx = model_info["sizes"].index(model_info["default_size"])
            except:
                pass
        
        model_size = st.selectbox(
            f"Ch·ªçn k√≠ch th∆∞·ªõc {model_info['name']}:",
            model_info["sizes"],
            index=default_idx,
            help="Model l·ªõn h∆°n th∆∞·ªùng ch√≠nh x√°c h∆°n nh∆∞ng ch·∫≠m h∆°n"
        )
    elif model_info and model_info.get("sizes"):
        model_size = model_info["sizes"][0]  # Use default/only size
    
    with_timestamps = st.checkbox("Hi·ªÉn th·ªã timestamps", value=True)
    enable_chunk = st.checkbox("X·ª≠ l√Ω audio d√†i b·∫±ng chunking", value=True)
    chunk_seconds = st.selectbox(
        "ƒê·ªô d√†i m·ªói chunk (gi√¢y)",
        [15, 30, 45, 60, 90, 120],
        index=2,
        help="Chia audio d√†i th√†nh c√°c ƒëo·∫°n nh·ªè ƒë·ªÉ tr√°nh h·∫øt b·ªô nh·ªõ",
    )
    auto_punct = st.checkbox("T·ª± ƒë·ªông ch√®n d·∫•u c√¢u (ƒë∆°n gi·∫£n)", value=False)
    
    if st.button("üöÄ B·∫Øt ƒë·∫ßu Transcription", type="primary"):
        if st.session_state.audio_data is not None:
            if not is_available:
                st.error("‚ùå Model ch∆∞a s·∫µn s√†ng. Vui l√≤ng c√†i ƒë·∫∑t dependencies tr∆∞·ªõc.")
            else:
                with st.spinner(f"ƒêang transcribe v·ªõi {model_info['name']}... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)"):
                    result = None
                    model_obj = None
                    transcripts = []
                    
                    # Load v√† transcribe theo t·ª´ng backend (c√≥ h·ªó tr·ª£ chunk cho Whisper/PhoWhisper)
                    try:
                        def transcribe_chunked_with_whisper(model_obj, language):
                            ranges = chunk_signal(
                                st.session_state.audio_data, st.session_state.audio_sr, int(chunk_seconds)
                            ) if enable_chunk else [(0, len(st.session_state.audio_data))]
                            progress = st.progress(0.0)
                            temp_files = []  # Track all temp files for cleanup
                            try:
                                for idx, (s0, s1) in enumerate(ranges, start=1):
                                    chunk_y = st.session_state.audio_data[s0:s1]
                                    # Create and close temp file first to avoid Windows file lock
                                    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                                        tmp_name = tmp_file.name
                                    temp_files.append(tmp_name)  # Track for cleanup
                                    sf.write(tmp_name, chunk_y, st.session_state.audio_sr)
                                    chunk_res = transcribe_audio(
                                        model_obj, tmp_name, sr=st.session_state.audio_sr,
                                        language=language, task="transcribe"
                                    )
                                    # Clean up immediately after use
                                    try:
                                        os.unlink(tmp_name)
                                        temp_files.remove(tmp_name)
                                    except:
                                        pass
                                    if chunk_res and chunk_res.get("text"):
                                        start_ts = format_timestamp(s0 / st.session_state.audio_sr)
                                        end_ts = format_timestamp(s1 / st.session_state.audio_sr)
                                        transcripts.append(f"[{start_ts} - {end_ts}] {chunk_res.get('text','').strip()}")
                                    progress.progress(idx / len(ranges))
                                return {"text": "\n".join(transcripts), "segments": []}
                            finally:
                                # Ensure all temp files are cleaned up even if there's an error
                                for tmp_name in temp_files:
                                    try:
                                        if os.path.exists(tmp_name):
                                            os.unlink(tmp_name)
                                    except:
                                        pass

                        def transcribe_chunked_with_phowhisper(model_obj):
                            ranges = chunk_signal(
                                st.session_state.audio_data, st.session_state.audio_sr, int(chunk_seconds)
                            ) if enable_chunk else [(0, len(st.session_state.audio_data))]
                            progress = st.progress(0.0)
                            temp_files = []  # Track all temp files for cleanup
                            try:
                                for idx, (s0, s1) in enumerate(ranges, start=1):
                                    chunk_y = st.session_state.audio_data[s0:s1]
                                    # Create and close temp file first to avoid Windows file lock
                                    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                                        tmp_name = tmp_file.name
                                    temp_files.append(tmp_name)  # Track for cleanup
                                    sf.write(tmp_name, chunk_y, st.session_state.audio_sr)
                                    chunk_res = transcribe_phowhisper(
                                        model_obj, tmp_name, sr=st.session_state.audio_sr, language="vi"
                                    )
                                    # Clean up immediately after use
                                    try:
                                        os.unlink(tmp_name)
                                        temp_files.remove(tmp_name)
                                    except:
                                        pass
                                    if chunk_res and chunk_res.get("text"):
                                        start_ts = format_timestamp(s0 / st.session_state.audio_sr)
                                        end_ts = format_timestamp(s1 / st.session_state.audio_sr)
                                        transcripts.append(f"[{start_ts} - {end_ts}] {chunk_res.get('text','').strip()}")
                                    progress.progress(idx / len(ranges))
                                return {"text": "\n".join(transcripts), "segments": []}
                            finally:
                                # Ensure all temp files are cleaned up even if there's an error
                                for tmp_name in temp_files:
                                    try:
                                        if os.path.exists(tmp_name):
                                            os.unlink(tmp_name)
                                    except:
                                        pass

                        if selected_model_id == "whisper":
                            model_obj, device = load_whisper_model(model_size)
                            if model_obj:
                                result = transcribe_chunked_with_whisper(model_obj, language="vi")
                            else:
                                st.error("‚ùå Kh√¥ng th·ªÉ load Whisper model!")
                        
                        elif selected_model_id == "phowhisper":
                            model_obj = load_phowhisper_model(model_size)
                            if model_obj:
                                result = transcribe_chunked_with_phowhisper(model_obj)
                            else:
                                st.error("‚ùå Kh√¥ng th·ªÉ load PhoWhisper model!")
                        
                        if result:
                            st.session_state.transcript_result = result
                            text_out = result.get("text", "") if isinstance(result, dict) else ""
                            if not text_out and hasattr(result, "text"):
                                text_out = result.text
                            if auto_punct:
                                # C·∫£i thi·ªán punctuation: th√™m d·∫•u c√¢u c∆° b·∫£n
                                text_out = text_out.replace(" ,", ",").replace(" .", ".")
                                # Th√™m d·∫•u ch·∫•m cu·ªëi c√¢u n·∫øu thi·∫øu
                                if text_out and text_out[-1] not in ".!?":
                                    text_out += "."
                                # Fix spacing around punctuation
                                text_out = re.sub(r'\s+([,.!?;:])', r'\1', text_out)
                                text_out = re.sub(r'([,.!?;:])\s*([,.!?;:])', r'\1\2', text_out)
                            if with_timestamps:
                                st.session_state.transcript_text = text_out or format_transcript(
                                    result, with_timestamps=True
                                )
                            else:
                                st.session_state.transcript_text = text_out or format_transcript(
                                    result, with_timestamps=False
                                )
                            st.success("‚úÖ Transcription ho√†n t·∫•t!")
                            st.rerun()
                        elif model_obj is None:
                            st.error("‚ùå Kh√¥ng th·ªÉ load model!")
                    
                    except Exception as e:
                        st.error(f"‚ùå L·ªói khi transcribe: {str(e)}")
        else:
            st.warning("‚ö†Ô∏è Vui l√≤ng upload file audio tr∆∞·ªõc!")
    
    # Hi·ªÉn th·ªã transcript
    if st.session_state.transcript_text:
        st.subheader("üìù Transcript")
        st.text_area(
            "K·∫øt qu·∫£ transcription:",
            st.session_state.transcript_text,
            height=400,
            key="transcript_display"
        )
        
        # Edit transcript
        st.subheader("‚úèÔ∏è Ch·ªânh s·ª≠a Transcript")
        edited_text = st.text_area(
            "Ch·ªânh s·ª≠a transcript:",
            st.session_state.transcript_text,
            height=300,
            key="transcript_edit"
        )
        
        if st.button("üíæ L∆∞u thay ƒë·ªïi"):
            st.session_state.transcript_text = edited_text
            st.success("‚úÖ ƒê√£ l∆∞u thay ƒë·ªïi!")
            st.rerun()

        st.subheader("üìä Th·ªëng k√™ & Export")
        words = st.session_state.transcript_text.split()
        word_count = len(words)
        duration = st.session_state.audio_info.get("duration") if st.session_state.audio_info else 0
        wpm = (word_count / duration * 60) if duration else 0
        col_stats = st.columns(3)
        col_stats[0].metric("S·ªë t·ª´", f"{word_count}")
        col_stats[1].metric("Th·ªùi l∆∞·ª£ng (s)", f"{duration:.2f}" if duration else "-")
        col_stats[2].metric("WPM", f"{wpm:.1f}")

        st.download_button(
            "‚¨áÔ∏è T·∫£i TXT",
            data=st.session_state.transcript_text,
            file_name="transcript.txt",
            mime="text/plain",
        )
