"""
ASR Benchmark Page
"""
import streamlit as st
import os
import sys
import subprocess

# Add parent directory to path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from app.components.sidebar import render_sidebar
from app.components.layout import apply_custom_css

# Apply custom CSS
apply_custom_css()

# Render sidebar with logo
render_sidebar()

st.header("üî¨ ASR Model Benchmark")
st.markdown("""
### So s√°nh nhi·ªÅu m√¥ h√¨nh ASR

Trang n√†y cho ph√©p b·∫°n ch·∫°y ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ƒë·ªÉ so s√°nh gi·ªØa c√°c m√¥ h√¨nh ASR kh√°c nhau.

**Y√™u c·∫ßu:**
- T·∫°o th∆∞ m·ª•c `test_audio/` trong project root
- Th√™m c√°c file audio test (.wav, .mp3, etc.)
- M·ªói file audio c·∫ßn c√≥ file `.txt` t∆∞∆°ng ·ª©ng ch·ª©a reference text (ground truth)
- V√≠ d·ª•: `audio1.wav` c·∫ßn c√≥ `audio1.txt`

**Metrics:**
- **WER (Word Error Rate)**: T·ª∑ l·ªá l·ªói t·ª´
- **CER (Character Error Rate)**: T·ª∑ l·ªá l·ªói k√Ω t·ª±

Gi√° tr·ªã th·∫•p h∆°n = t·ªët h∆°n.
""")

from core.asr.model_registry import get_all_models, get_model_info, check_model_dependencies

st.subheader("‚öôÔ∏è C·∫•u h√¨nh ƒë√°nh gi√°")

all_models = get_all_models()

# Model selection - allow multiple models
st.markdown("**Ch·ªçn c√°c m√¥ h√¨nh ƒë·ªÉ so s√°nh:**")
selected_models = st.multiselect(
    "Models:",
    options=list(all_models.keys()),
    default=["whisper", "phowhisper"],
    format_func=lambda x: all_models[x]["name"]
)

# Model sizes for each selected model
model_configs = {}
for model_id in selected_models:
    model_info = get_model_info(model_id)
    if model_info and model_info.get("sizes"):
        if len(model_info["sizes"]) > 1:
            size = st.selectbox(
                f"{model_info['name']} size:",
                model_info["sizes"],
                key=f"size_{model_id}",
                index=model_info["sizes"].index(model_info.get("default_size", model_info["sizes"][0])) if model_info.get("default_size") in model_info["sizes"] else 0
            )
        else:
            size = model_info["sizes"][0]
        model_configs[model_id] = size
    else:
        model_configs[model_id] = "default"

test_dir = st.text_input("Th∆∞ m·ª•c test audio:", value="test_audio")

if st.button("üöÄ Ch·∫°y ƒë√°nh gi√°", type="primary"):
    if not selected_models:
        st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt m√¥ h√¨nh ƒë·ªÉ ƒë√°nh gi√°!")
    else:
        # Check if test directory exists
        test_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), test_dir)
        
        if not os.path.exists(test_path):
            st.error(f"‚ùå Th∆∞ m·ª•c {test_dir} kh√¥ng t·ªìn t·∫°i!")
        else:
            # Check dependencies for selected models
            unavailable_models = []
            for model_id in selected_models:
                is_available, missing = check_model_dependencies(model_id)
                if not is_available:
                    unavailable_models.append((model_id, missing))
            
            if unavailable_models:
                st.warning("‚ö†Ô∏è M·ªôt s·ªë m√¥ h√¨nh ch∆∞a s·∫µn s√†ng:")
                for model_id, missing in unavailable_models:
                    model_name = get_model_info(model_id)["name"]
                    st.write(f"- {model_name}: Thi·∫øu {', '.join(missing)}")
                st.info("üí° B·∫°n v·∫´n c√≥ th·ªÉ ch·∫°y ƒë√°nh gi√° v·ªõi c√°c m√¥ h√¨nh ƒë√£ s·∫µn s√†ng.")
            
            with st.spinner("ƒêang ch·∫°y ƒë√°nh gi√°... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)"):
                st.info("üí° T√≠nh nƒÉng benchmark ƒë·∫ßy ƒë·ªß ƒëang ƒë∆∞·ª£c ph√°t tri·ªÉn. Hi·ªán t·∫°i h·ªó tr·ª£ Whisper v√† PhoWhisper.")
                try:
                    # Run evaluation script (currently supports Whisper and PhoWhisper)
                    script_path = os.path.join(
                        os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                        "core", "asr", "evaluate_models.py"
                    )
                    
                    # Filter to supported models for now
                    supported = [m for m in selected_models if m in ["whisper", "phowhisper"]]
                    if not supported:
                        st.error("‚ùå Benchmark script hi·ªán ch·ªâ h·ªó tr·ª£ Whisper v√† PhoWhisper.")
                    else:
                        whisper_size = model_configs.get("whisper", "large") if "whisper" in supported else "large"
                        phowhisper_size = model_configs.get("phowhisper", "medium") if "phowhisper" in supported else "medium"
                        
                        result = subprocess.run(
                            [
                                sys.executable,
                                script_path,
                                "--test_dir", test_dir,
                                "--whisper_model", whisper_size,
                                "--phowhisper_model", phowhisper_size,
                                "--output", "docs/model_comparison.md"
                            ],
                            capture_output=True,
                            text=True,
                            cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
                        )
                        
                        if result.returncode == 0:
                            st.success("‚úÖ ƒê√°nh gi√° ho√†n t·∫•t!")
                            st.code(result.stdout)
                            
                            # Show report
                            report_path = os.path.join(
                                os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                                "docs", "model_comparison.md"
                            )
                            if os.path.exists(report_path):
                                with open(report_path, 'r', encoding='utf-8') as f:
                                    st.markdown(f.read())
                        else:
                            st.error(f"‚ùå L·ªói khi ch·∫°y ƒë√°nh gi√°:\n{result.stderr}")
                except Exception as e:
                    st.error(f"‚ùå L·ªói: {str(e)}")

# Show existing report if available
report_path = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
    "docs", "model_comparison.md"
)
if os.path.exists(report_path):
    st.subheader("üìÑ B√°o c√°o hi·ªán c√≥")
    with open(report_path, 'r', encoding='utf-8') as f:
        st.markdown(f.read())

