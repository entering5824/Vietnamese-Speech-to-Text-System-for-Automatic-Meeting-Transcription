"""
H·ªá th·ªëng Chuy·ªÉn Gi·ªçng N√≥i Ti·∫øng Vi·ªát Sang VƒÉn B·∫£n
Vietnamese Speech to Text System for Automatic Meeting Transcription
"""
import streamlit as st
import numpy as np
import tempfile
import os
from datetime import datetime
import io
import soundfile as sf

# Setup static FFmpeg tr∆∞·ªõc khi import c√°c module kh√°c
# Silent mode ƒë·ªÉ tr√°nh hi·ªÉn th·ªã th√¥ng b√°o khi ch∆∞a c√≥ Streamlit context
from ffmpeg_setup import ensure_ffmpeg
ensure_ffmpeg(silent=True)

# Import c√°c module t·ª± t·∫°o
from audio_processor import (
    load_audio, preprocess_audio, plot_waveform, 
    plot_spectrogram, get_audio_info
)
from transcription_service import (
    load_whisper_model, transcribe_audio, format_transcript,
    format_time, get_transcript_statistics
)
from export_utils import export_txt, export_docx, export_pdf
from speaker_diarization import simple_speaker_segmentation, format_with_speakers

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="Vietnamese Speech to Text",
    page_icon="üé§",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS t√πy ch·ªânh
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f4e79;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stat-box {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f4e79;
    }
    </style>
""", unsafe_allow_html=True)

# Sidebar
st.sidebar.title("üé§ Vietnamese Speech to Text")
st.sidebar.markdown("---")

# Menu ƒëi·ªÅu h∆∞·ªõng
page = st.sidebar.radio(
    "Ch·ªçn ch·ª©c nƒÉng:",
    [
        "üè† Trang ch·ªß", 
        "üì§ Upload & Transcribe", 
        "üéôÔ∏è Ghi √¢m tr·ª±c ti·∫øp", 
        "üìä Th·ªëng k√™ & Export",
        "üñºÔ∏è Image Encryption"
    ]
)

# Initialize session state
if 'audio_data' not in st.session_state:
    st.session_state.audio_data = None
if 'audio_sr' not in st.session_state:
    st.session_state.audio_sr = None
if 'transcript_result' not in st.session_state:
    st.session_state.transcript_result = None
if 'transcript_text' not in st.session_state:
    st.session_state.transcript_text = ""
if 'audio_info' not in st.session_state:
    st.session_state.audio_info = None

# ========== TRANG CH·ª¶ ==========
if page == "üè† Trang ch·ªß":
    st.markdown('<div class="main-header">Topic 7. Designing and Developing a Vietnamese Speech to Text System for Automatic Meeting Transcription</div>', 
                unsafe_allow_html=True)
    
    st.markdown("""
    ### üìã Gi·ªõi thi·ªáu
    
    H·ªá th·ªëng n√†y cho ph√©p b·∫°n chuy·ªÉn ƒë·ªïi gi·ªçng n√≥i ti·∫øng Vi·ªát th√†nh vƒÉn b·∫£n m·ªôt c√°ch t·ª± ƒë·ªông v√† ch√≠nh x√°c.
    H·ªá th·ªëng h·ªó tr·ª£:
    
    - ‚úÖ Upload file audio (WAV, MP3, FLAC)
    - ‚úÖ Ghi √¢m tr·ª±c ti·∫øp t·ª´ microphone
    - ‚úÖ X·ª≠ l√Ω audio d√†i (meetings, interviews)
    - ‚úÖ Visualize waveform v√† spectrogram
    - ‚úÖ Ti·ªÅn x·ª≠ l√Ω audio (normalize, noise reduction)
    - ‚úÖ Transcription v·ªõi timestamps
    - ‚úÖ Speaker diarization (ph√¢n bi·ªát ng∆∞·ªùi n√≥i)
    - ‚úÖ Export ra TXT, DOCX, PDF
    - ‚úÖ Th·ªëng k√™ chi ti·∫øt
    
    ### üöÄ B·∫Øt ƒë·∫ßu
    
    1. Ch·ªçn **"Upload & Transcribe"** ƒë·ªÉ upload file audio
    2. Ho·∫∑c ch·ªçn **"Ghi √¢m tr·ª±c ti·∫øp"** ƒë·ªÉ ghi √¢m t·ª´ microphone
    3. Xem k·∫øt qu·∫£ v√† export n·∫øu c·∫ßn
    
    ### üîß C√¥ng ngh·ªá s·ª≠ d·ª•ng
    
    - **Speech Recognition**: OpenAI Whisper
    - **Audio Processing**: Librosa, PyDub, SoundFile
    - **Visualization**: Matplotlib, Seaborn
    - **Framework**: Streamlit
    """)
    
    st.markdown("---")
    st.caption("Made with ‚ù§Ô∏è using Streamlit and Whisper")

# ========== UPLOAD & TRANSCRIBE ==========
elif page == "üì§ Upload & Transcribe":
    st.header("üì§ Upload & Transcribe Audio")
    
    # Upload file
    uploaded_file = st.file_uploader(
        "Ch·ªçn file audio (WAV, MP3, FLAC)",
        type=['wav', 'mp3', 'flac', 'm4a', 'ogg'],
        help="H·ªó tr·ª£ c√°c ƒë·ªãnh d·∫°ng: WAV, MP3, FLAC, M4A, OGG"
    )
    
    if uploaded_file is not None:
        # Load audio
        with st.spinner("ƒêang t·∫£i audio..."):
            audio_data, sr = load_audio(uploaded_file)
            
            if audio_data is not None:
                st.session_state.audio_data = audio_data
                st.session_state.audio_sr = sr
                st.session_state.audio_info = get_audio_info(audio_data, sr)
                
                st.success(f"‚úÖ ƒê√£ t·∫£i audio th√†nh c√¥ng!")
                
                # Hi·ªÉn th·ªã th√¥ng tin audio
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ƒê·ªô d√†i", f"{st.session_state.audio_info['duration']:.2f} gi√¢y")
                with col2:
                    st.metric("Sample Rate", f"{sr} Hz")
                with col3:
                    st.metric("S·ªë m·∫´u", f"{len(audio_data):,}")
        
        # Preprocessing options
        st.subheader("‚öôÔ∏è Ti·ªÅn x·ª≠ l√Ω Audio")
        col1, col2 = st.columns(2)
        with col1:
            normalize = st.checkbox("Normalize audio", value=True)
        with col2:
            remove_noise = st.checkbox("Lo·∫°i b·ªè noise", value=False)
        
        if st.button("√Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω"):
            if st.session_state.audio_data is not None:
                with st.spinner("ƒêang x·ª≠ l√Ω..."):
                    st.session_state.audio_data = preprocess_audio(
                        st.session_state.audio_data, 
                        st.session_state.audio_sr,
                        normalize=normalize,
                        remove_noise=remove_noise
                    )
                st.success("‚úÖ ƒê√£ √°p d·ª•ng ti·ªÅn x·ª≠ l√Ω!")
        
        # Visualization
        if st.session_state.audio_data is not None:
            st.subheader("üìä Visualization")
            viz_option = st.radio(
                "Ch·ªçn lo·∫°i visualization:",
                ["Waveform", "Spectrogram", "C·∫£ hai"],
                horizontal=True
            )
            
            if viz_option in ["Waveform", "C·∫£ hai"]:
                fig_wave = plot_waveform(st.session_state.audio_data, st.session_state.audio_sr)
                st.pyplot(fig_wave)
            
            if viz_option in ["Spectrogram", "C·∫£ hai"]:
                fig_spec = plot_spectrogram(st.session_state.audio_data, st.session_state.audio_sr)
                st.pyplot(fig_spec)
        
        # Transcription
        st.subheader("üéØ Transcription")
        
        col1, col2 = st.columns(2)
        with col1:
            model_size = st.selectbox(
                "Ch·ªçn model Whisper:",
                ["tiny", "base", "small", "medium", "large"],
                index=1,
                help="Model l·ªõn h∆°n = ch√≠nh x√°c h∆°n nh∆∞ng ch·∫≠m h∆°n"
            )
        with col2:
            with_timestamps = st.checkbox("Hi·ªÉn th·ªã timestamps", value=True)
        
        speaker_diarization = st.checkbox("Speaker Diarization (ph√¢n bi·ªát ng∆∞·ªùi n√≥i)", value=False)
        
        if st.button("üöÄ B·∫Øt ƒë·∫ßu Transcription", type="primary"):
            if st.session_state.audio_data is not None:
                with st.spinner(f"ƒêang transcribe v·ªõi model {model_size}... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)"):
                    # Load model
                    model, device = load_whisper_model(model_size)
                    
                    if model is not None:
                        # L∆∞u audio v√†o temp file
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                            sf.write(tmp_file.name, st.session_state.audio_data, st.session_state.audio_sr)
                            
                            # Transcribe
                            result = transcribe_audio(
                                model,
                                tmp_file.name,
                                sr=st.session_state.audio_sr,
                                language="vi",
                                task="transcribe"
                            )
                            
                            # Clean up
                            os.unlink(tmp_file.name)
                            
                            if result:
                                st.session_state.transcript_result = result
                                
                                # Format transcript
                                if speaker_diarization and 'segments' in result:
                                    # Simple speaker segmentation
                                    speaker_segments = simple_speaker_segmentation(
                                        st.session_state.audio_data,
                                        st.session_state.audio_sr,
                                        result['segments']
                                    )
                                    if speaker_segments:
                                        st.session_state.transcript_text = format_with_speakers(speaker_segments)
                                    else:
                                        st.session_state.transcript_text = format_transcript(
                                            result, with_timestamps=with_timestamps
                                        )
                                else:
                                    st.session_state.transcript_text = format_transcript(
                                        result, with_timestamps=with_timestamps
                                    )
                                
                                st.success("‚úÖ Transcription ho√†n t·∫•t!")
                            else:
                                st.error("‚ùå L·ªói khi transcribe!")
                    else:
                        st.error("‚ùå Kh√¥ng th·ªÉ load model!")
            else:
                st.warning("‚ö†Ô∏è Vui l√≤ng upload file audio tr∆∞·ªõc!")
        
        # Hi·ªÉn th·ªã transcript
        if st.session_state.transcript_text:
            st.subheader("üìù Transcript")
            st.text_area(
                "K·∫øt qu·∫£ transcription:",
                st.session_state.transcript_text,
                height=400,
                key="transcript_display"
            )
            
            # Edit transcript
            st.subheader("‚úèÔ∏è Ch·ªânh s·ª≠a Transcript")
            edited_text = st.text_area(
                "Ch·ªânh s·ª≠a transcript:",
                st.session_state.transcript_text,
                height=300,
                key="transcript_edit"
            )
            
            if st.button("üíæ L∆∞u thay ƒë·ªïi"):
                st.session_state.transcript_text = edited_text
                st.success("‚úÖ ƒê√£ l∆∞u thay ƒë·ªïi!")

# ========== GHI √ÇM TR·ª∞C TI·∫æP ==========
elif page == "üéôÔ∏è Ghi √¢m tr·ª±c ti·∫øp":
    st.header("üéôÔ∏è Ghi √¢m tr·ª±c ti·∫øp")
    
    st.info("üí° T√≠nh nƒÉng n√†y cho ph√©p b·∫°n upload file audio ƒë√£ ghi √¢m s·∫µn ƒë·ªÉ transcribe ngay l·∫≠p t·ª©c.")
    st.warning("‚ö†Ô∏è ƒê·ªÉ ghi √¢m tr·ª±c ti·∫øp, vui l√≤ng s·ª≠ d·ª•ng ·ª©ng d·ª•ng ghi √¢m tr√™n m√°y t√≠nh ho·∫∑c ƒëi·ªán tho·∫°i, sau ƒë√≥ upload file t·∫°i ƒë√¢y.")
    
    # Audio upload cho recording
    audio_file = st.file_uploader(
        "Upload file audio ƒë√£ ghi √¢m:",
        type=['wav', 'mp3', 'flac', 'm4a', 'ogg'],
        key="recording_upload"
    )
    
    if audio_file:
        st.success("‚úÖ ƒê√£ t·∫£i file audio th√†nh c√¥ng!")
        
        # Play audio
        st.audio(audio_file, format='audio/wav')
        
        # Load audio t·ª´ file
        with st.spinner("ƒêang x·ª≠ l√Ω audio..."):
            audio_data, sr = load_audio(audio_file)
            
            if audio_data is not None:
                st.session_state.audio_data = audio_data
                st.session_state.audio_sr = sr
                st.session_state.audio_info = get_audio_info(audio_data, sr)
                
                # Hi·ªÉn th·ªã th√¥ng tin
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("ƒê·ªô d√†i", f"{st.session_state.audio_info['duration']:.2f} gi√¢y")
                with col2:
                    st.metric("Sample Rate", f"{sr} Hz")
        
        # Transcription
        model_size = st.selectbox(
            "Ch·ªçn model Whisper:",
            ["tiny", "base", "small", "medium"],
            index=1
        )
        
        if st.button("üöÄ Transcribe", type="primary"):
            if st.session_state.audio_data is not None:
                with st.spinner("ƒêang transcribe..."):
                    model, device = load_whisper_model(model_size)
                    
                    if model is not None:
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                            sf.write(tmp_file.name, st.session_state.audio_data, st.session_state.audio_sr)
                            
                            result = transcribe_audio(
                                model,
                                tmp_file.name,
                                sr=st.session_state.audio_sr,
                                language="vi"
                            )
                            
                            os.unlink(tmp_file.name)
                            
                            if result:
                                st.session_state.transcript_result = result
                                st.session_state.transcript_text = format_transcript(result, with_timestamps=True)
                                st.success("‚úÖ Transcription ho√†n t·∫•t!")
                                
                                st.text_area(
                                    "Transcript:",
                                    st.session_state.transcript_text,
                                    height=300
                                )

# ========== TH·ªêNG K√ä & EXPORT ==========
elif page == "üìä Th·ªëng k√™ & Export":
    st.header("üìä Th·ªëng k√™ & Export")
    
    if st.session_state.transcript_result and st.session_state.audio_info:
        # Statistics
        st.subheader("üìà Th·ªëng k√™")
        
        stats = get_transcript_statistics(
            st.session_state.transcript_result,
            st.session_state.audio_info['duration']
        )
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("S·ªë t·ª´", f"{stats.get('word_count', 0):,}")
        with col2:
            st.metric("S·ªë k√Ω t·ª±", f"{stats.get('character_count', 0):,}")
        with col3:
            st.metric("ƒê·ªô d√†i", f"{stats.get('duration', 0):.2f} gi√¢y")
        with col4:
            st.metric("T·ª´/ph√∫t", f"{stats.get('words_per_minute', 0):.1f}")
        
        # Export options
        st.subheader("üíæ Export Transcript")
        
        export_format = st.radio(
            "Ch·ªçn ƒë·ªãnh d·∫°ng export:",
            ["TXT", "DOCX", "PDF"],
            horizontal=True
        )
        
        # Metadata
        metadata = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'duration': stats.get('duration', 0),
            'word_count': stats.get('word_count', 0),
            'character_count': stats.get('character_count', 0)
        }
        
        if st.button(f"üì• Export {export_format}", type="primary"):
            if st.session_state.transcript_text:
                try:
                    if export_format == "TXT":
                        file_bytes, filename = export_txt(
                            st.session_state.transcript_text,
                            f"transcript_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                        )
                        st.download_button(
                            label="‚¨áÔ∏è T·∫£i xu·ªëng TXT",
                            data=file_bytes,
                            file_name=filename,
                            mime="text/plain"
                        )
                    
                    elif export_format == "DOCX":
                        file_bytes, filename = export_docx(
                            st.session_state.transcript_text,
                            metadata,
                            f"transcript_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
                        )
                        st.download_button(
                            label="‚¨áÔ∏è T·∫£i xu·ªëng DOCX",
                            data=file_bytes,
                            file_name=filename,
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                        )
                    
                    elif export_format == "PDF":
                        file_bytes, filename = export_pdf(
                            st.session_state.transcript_text,
                            metadata,
                            f"transcript_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
                        )
                        st.download_button(
                            label="‚¨áÔ∏è T·∫£i xu·ªëng PDF",
                            data=file_bytes,
                            file_name=filename,
                            mime="application/pdf"
                        )
                    
                    st.success("‚úÖ File ƒë√£ s·∫µn s√†ng ƒë·ªÉ t·∫£i xu·ªëng!")
                except Exception as e:
                    st.error(f"‚ùå L·ªói khi export: {str(e)}")
            else:
                st.warning("‚ö†Ô∏è Kh√¥ng c√≥ transcript ƒë·ªÉ export!")
    else:
        st.info("‚ÑπÔ∏è Vui l√≤ng transcribe audio tr∆∞·ªõc ƒë·ªÉ xem th·ªëng k√™ v√† export.")

# ========== IMAGE ENCRYPTION PAGE ==========
elif page == "üñºÔ∏è Image Encryption":
    st.header("üñºÔ∏è Image Encryption (Password Protected)")

    st.write("M√£ h√≥a / gi·∫£i m√£ ·∫£nh b·∫±ng password, chaotic logistic map v√† patch-level XOR.")

    import numpy as np
    from PIL import Image
    import hashlib
    import io

    # ===== KEY DERIVATION =====
    def derive_keys(password: str):
        h = hashlib.sha256(password.encode()).digest()
        seed = (int.from_bytes(h[:4], "big") % 1_000_000) / 1_000_000
        r = 3.8 + (h[4] / 255) * 0.19
        patch_size = [8, 16, 32][h[-1] % 3]
        xor_key = np.frombuffer(h, dtype=np.uint8)
        return seed, r, patch_size, xor_key

    # ===== CHAOTIC MAP =====
    def logistic_map(seed, r, size):
        x = seed
        arr = np.zeros(size)
        for i in range(size):
            x = r * x * (1 - x)
            arr[i] = x
        return arr

    # ===== PATCHIFY =====
    def patchify(img, patch):
        h, w, c = img.shape
        assert h % patch == 0 and w % patch == 0
        return (
            img.reshape(h//patch, patch, w//patch, patch, c)
               .swapaxes(1, 2)
               .reshape(-1, patch, patch, c)
        )

    def unpatchify(patches, img_shape, patch):
        h, w, c = img_shape
        H, W = h//patch, w//patch
        return (patches.reshape(H, W, patch, patch, c)
                      .swapaxes(1, 2)
                      .reshape(h, w, c))

    # ===== ENCRYPT =====
    def encrypt(img, password):
        seed, r, patch, xor_key = derive_keys(password)
        patches = patchify(img, patch)
        N = len(patches)

        chaos = logistic_map(seed, r, N)
        perm = np.argsort(chaos)
        chaos_vals = (chaos * 255).astype(np.uint8)

        enc = []
        for i in range(N):
            p = patches[i].astype(np.uint8)
            key = chaos_vals[i] ^ xor_key[i % len(xor_key)]
            enc.append(p ^ key)

        enc = np.stack(enc)[perm]
        return unpatchify(enc, img.shape, patch)

    # ===== DECRYPT =====
    def decrypt(img, password):
        seed, r, patch, xor_key = derive_keys(password)
        patches = patchify(img, patch)
        N = len(patches)

        chaos = logistic_map(seed, r, N)
        perm = np.argsort(chaos)
        inv_perm = np.argsort(perm)
        chaos_vals = (chaos * 255).astype(np.uint8)

        dec = np.zeros_like(patches)
        for i in range(N):
            p = patches[inv_perm[i]].astype(np.uint8)
            key = chaos_vals[i] ^ xor_key[i % len(xor_key)]
            dec[i] = p ^ key

        return unpatchify(dec, img.shape, patch)

    # ===== UI =====

    uploaded = st.file_uploader("Upload ·∫£nh PNG/JPG", type=["png", "jpg", "jpeg"])
    password = st.text_input("Nh·∫≠p m·∫≠t kh·∫©u", type="password")
    mode = st.selectbox("Ch·∫ø ƒë·ªô:", ["Encrypt", "Decrypt"])

    if uploaded:
        img = Image.open(uploaded).convert("RGB")
        img = img.resize((256, 256))
        arr = np.array(img)

        st.image(img, caption="·∫¢nh ƒë·∫ßu v√†o", use_column_width=True)

        if st.button("‚ñ∂Ô∏è Run Encryption/Decryption"):
            if not password:
                st.error("Vui l√≤ng nh·∫≠p m·∫≠t kh·∫©u!")
            else:
                if mode == "Encrypt":
                    out = encrypt(arr, password)
                else:
                    out = decrypt(arr, password)

                st.image(out, caption="·∫¢nh output", use_column_width=True)

                buffer = io.BytesIO()
                Image.fromarray(out).save(buffer, format="PNG")

                st.download_button(
                    "‚¨á T·∫£i ·∫£nh",
                    buffer.getvalue(),
                    "output.png",
                    "image/png"
                )
# Footer
st.markdown("---")
st.caption("Vietnamese Speech to Text System | Made with Streamlit & Whisper")
