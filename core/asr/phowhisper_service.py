"""
Module transcription s·ª≠ d·ª•ng PhoWhisper (HuggingFace)
PhoWhisper l√† m√¥ h√¨nh ASR t·ªëi ∆∞u cho ti·∫øng Vi·ªát t·ª´ VinAI Research
"""
import torch
import streamlit as st
from typing import Optional, Dict, List
import numpy as np
from transformers import pipeline
import librosa
import soundfile as sf
import tempfile
import os

@st.cache_resource
def load_phowhisper_model(model_size="small"):
    """
    Load PhoWhisper model t·ª´ HuggingFace v·ªõi cache
    
    Args:
        model_size: "small", "medium", ho·∫∑c "base"
    
    Returns:
        pipeline: Transformers pipeline object ho·∫∑c None n·∫øu l·ªói
    """
    try:
        # Ki·ªÉm tra v√† c√†i ƒë·∫∑t tf-keras n·∫øu c·∫ßn (fix Keras 3 compatibility)
        try:
            import tf_keras
        except ImportError:
            try:
                import subprocess
                import sys
                st.warning("‚ö†Ô∏è ƒêang c√†i ƒë·∫∑t tf-keras ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi Keras 3...")
                subprocess.check_call([sys.executable, "-m", "pip", "install", "tf-keras>=2.15.0", "-q"])
                import tf_keras
            except Exception as install_error:
                st.error(f"‚ùå Kh√¥ng th·ªÉ c√†i ƒë·∫∑t tf-keras: {str(install_error)}")
                st.info("üí° Vui l√≤ng c√†i ƒë·∫∑t th·ªß c√¥ng: pip install tf-keras")
                return None
        
        device = 0 if torch.cuda.is_available() else -1
        model_name = f"vinai/PhoWhisper-{model_size}"
        
        # Load pipeline
        transcriber = pipeline(
            "automatic-speech-recognition",
            model=model_name,
            device=device
        )
        
        return transcriber
    except Exception as e:
        error_msg = str(e)
        if "Keras" in error_msg or "tf-keras" in error_msg:
            st.error(f"‚ùå L·ªói Keras compatibility: {error_msg}")
            st.info("üí° Vui l√≤ng c√†i ƒë·∫∑t: pip install tf-keras")
        else:
            st.error(f"L·ªói khi load PhoWhisper model: {error_msg}")
        return None

def transcribe_phowhisper(model, audio_path_or_array, sr=16000, language="vi"):
    """
    Transcribe audio s·ª≠ d·ª•ng PhoWhisper
    
    Args:
        model: PhoWhisper pipeline model
        audio_path_or_array: ƒê∆∞·ªùng d·∫´n file ho·∫∑c numpy array
        sr: Sample rate (PhoWhisper y√™u c·∫ßu 16kHz)
        language: Ng√¥n ng·ªØ (vi cho ti·∫øng Vi·ªát)
    
    Returns:
        Dict: K·∫øt qu·∫£ transcription v·ªõi format t∆∞∆°ng th√≠ch Whisper
        {
            "text": str,
            "segments": List[Dict] (c√≥ th·ªÉ r·ªóng n·∫øu kh√¥ng c√≥ timestamps)
        }
    """
    try:
        if model is None:
            return None
        
        # X·ª≠ l√Ω input: c√≥ th·ªÉ l√† file path ho·∫∑c numpy array
        is_temp = False
        if isinstance(audio_path_or_array, str):
            # ƒê√£ l√† file path
            audio_path = audio_path_or_array
        else:
            # L√† numpy array, c·∫ßn l∆∞u v√†o temp file
            with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                sf.write(tmp_file.name, audio_path_or_array, sr)
                audio_path = tmp_file.name
                is_temp = True
        
        # Transcribe v·ªõi PhoWhisper
        result = model(audio_path, return_timestamps=True)
        
        # Clean up temp file n·∫øu c√≥
        if is_temp and os.path.exists(audio_path):
            os.unlink(audio_path)
        
        # Format k·∫øt qu·∫£ ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi Whisper format
        output = {
            "text": result.get("text", ""),
            "segments": []
        }
        
        # PhoWhisper c√≥ th·ªÉ tr·∫£ v·ªÅ chunks v·ªõi timestamps
        if "chunks" in result:
            for chunk in result["chunks"]:
                if "timestamp" in chunk:
                    timestamp = chunk["timestamp"]
                    output["segments"].append({
                        "start": timestamp[0] if isinstance(timestamp, (list, tuple)) else timestamp,
                        "end": timestamp[1] if isinstance(timestamp, (list, tuple)) and len(timestamp) > 1 else timestamp,
                        "text": chunk.get("text", "").strip()
                    })
        
        # N·∫øu kh√¥ng c√≥ chunks, t·∫°o m·ªôt segment duy nh·∫•t
        if not output["segments"] and output["text"]:
            # ∆Ø·ªõc t√≠nh duration t·ª´ audio file
            try:
                if isinstance(audio_path_or_array, str):
                    duration = librosa.get_duration(path=audio_path_or_array)
                else:
                    duration = len(audio_path_or_array) / sr
            except:
                duration = 0
            
            output["segments"] = [{
                "start": 0.0,
                "end": duration,
                "text": output["text"]
            }]
        
        return output
    except Exception as e:
        st.error(f"L·ªói khi transcribe v·ªõi PhoWhisper: {str(e)}")
        # Clean up temp file n·∫øu c√≥
        if 'is_temp' in locals() and is_temp and os.path.exists(audio_path):
            try:
                os.unlink(audio_path)
            except:
                pass
        return None

# T√°i s·ª≠ d·ª•ng c√°c h√†m format t·ª´ transcription_service
from .transcription_service import format_transcript, format_time, get_transcript_statistics

